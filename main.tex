% ###################### Form ######################
\documentclass{article}
\usepackage{geometry}

\geometry{
 a4paper,
 total={130mm,257mm},
 left=30mm,
 top=20mm
}

\usepackage{setspace}
\onehalfspacing

\usepackage[T1]{fontenc}
\usepackage{lmodern}

% ###################### Stil ######################
\usepackage[ngerman]{babel}
\usepackage{graphicx}

\usepackage{xurl} % korrekte Umbrüche in Hyperlinks
\usepackage[
    colorlinks=true, % verhindert Linkbox, aber färbt
    urlcolor=black, % deshalb alles schwarz
    linkcolor=black,
    citecolor=black,
    filecolor=black,
    breaklinks=true
]{hyperref} % korrekter Hyperlink, sogar bei Umbrüchen
% \expandafter\def\expandafter\UrlBreaks\expandafter{
%   \UrlBreaks
%   \do\- % erlaubt Umbrüche am Bindestrich
% }

\usepackage{dblfnote} % Footnotes in zwei Spalten
\usepackage{footmisc} % Footnotes referieren

% ##################### Befehle ####################
\newcommand{\term}[1]{\textbf{#1}} % term
\newcommand{\usfw}[1]{\textit{#1}} % umgangsprachlich/fremdwort

% These cite commands make for an easier switch between in-text and footnote citations
\newcommand{\ct}[2]{\footnote{\label{footnote:#1-#2}\cite[#2]{#1}}} % cite (normal: source, location)
\newcommand{\ctr}[1]{\footnote{\label{footnote:#1}\cite{#1}}} % cite ("raw": only source)
\usepackage{xparse}
% \NewDocumentCommand{\cts}{o m}{ % cite (multiple sources)
%   \IfNoValueTF{#1}
%     {\footnote{\cite{#2}}}%
%     {\footnote{\cite[#1]{#2}}}%
% }
\NewDocumentCommand{\ctsr}{m}{ % cite (multiple sources raw)
    \footnote{\cite{#1}}%
}
\newcommand{\ctm}[1]{\footnote{#1}} % cite manual [only footnote, `\cite` needs to be used]

\newcommand{\qt}[1]{\glqq{}#1\grqq} % quote

% #################### Metadaten ###################
\title{Untersuchung der Algorithmen zur Sprachkorrektur und -vorhersage am Beispiel der Bildschirmtastatur}
\author{Vorname Nachnahme}
\date{Oktober 2025}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

% ################### Titelblatt ###################
\maketitle %ggf ersetzen durch titlepage Umgebung

\pagebreak
% ############### Inhaltsverzeichnis ###############
\tableofcontents

\pagebreak
% ##################### Inhalt #####################

% =================== Anmerkungen ==================
\textbf{Im Folgenden sind jegliche maskulinen, femininen oder neutralen Wortformen, sofern der Kontext es zulässt und nicht explizit das Gegenteil angegeben ist, als generische Formen zu verstehen. Der Einfach- und Knappheit halber wird auf aufwändigere und in der Regel längere geschlechtsneutrale Formen verzichtet, auch mangels einer standardisierten Schreibweise.}

\textbf{Um Absätze (\usfw{Abs.}) handelt es sich in Quellen, wenn es einen Paragrafen von Text, einem oder mehreren nummerierten oder nicht nummerierten Stichpunkten im Textzusammenhang bezeichnet, der in der in dem speziellen Text üblichen Art und Weise -- vor allem im Abstand -- vom Rest des Textes abgegrenzt ist. Ausgeschlossen sind also zum Beispiel Tabellen, Bildunterschriften oder Verweise auf andere Artikel, die bestimmten Quellen inne sein mögen, zu ihnen aber keinen nennenswerten Mehrwert beitragen.}

% =================== Einleitung ===================
\section{Einleitung}
\label{sec:einleitung}

Beim Tippen auf Bildschirmtastaturen, wie sie auf modernen Mobiltelefonen oder Tabletcomputern mit berührungsempfindlichem Bildschirm zu finden sind, bleiben viele im Hintergrund ablaufende Prozesse für den Nutzer unsichtbar. Idealerweise ist die Benutzererfahrung mit einer digitalen Tastatur so problemlos, dass Korrekturmaßnahmen und Optimierungen nicht auffallen, die Nachteile der geringeren Größe im Vergleich zur physischen Tastatur ausgleichen sollen \ct{iphoneticker_iphone_entwicklung}{Abs. 5--6}.

In der Realität funktioniert das meist nicht so gut, wie zahlreiche Nutzerberichte und Diskussionen in sozialen Medien zeigen. Hier wird vor allem die vermeintliche Korrektur von bereits richtig geschriebenen Wörtern kritisiert. \ctsr{reddit_autokorrect_schlecht_1,reddit_autokorrect_schlecht_2}

\term{Autokorrektur}, wie der Algorithmus genannt wird, der Tippfehler korrigieren soll, ist schon für die erste wirklich erfolgreiche Bildschirmtastatur implementiert -- Tippfehler sind fast unvermeidbar. \ct{apple_iphone_praesentation_video}{31:21--31:35}

\term{Schreibvorschläge}, meist in einer Leiste über der eigentlichen Tastatur, sind ergänzend dazu die Norm. Sie geben dem Benutzer während des Tippens auswählbare Vorschläge, zu denen das momentan direkt vor, um oder nach dem Cursor befindliche Wort geändert werden kann, aber nicht muss. \ctm{\cite[Abs. 1,2]{techbone_android_vorschlaege_verwenden_einstellen}\cite[Abs. 2,6--7]{apple_autokorrektur_vorschlaege_verwenden}}

Sie sind besonders dann nützlich, wenn ein falsch geschriebenes Wort gleich ähnlich zu verschiedenen Worten ist, die auch alle kontextuell möglich und gleich wahrscheinlich sind: Autokorrektur müsste in dem Fall ein zufälliges Wort auswählen; der Nutzer weiß besser, was er tippen will.

Außerdem sind sie schon verfügbar, bevor überhaupt ein Buchstabe des nächsten Wortes getippt wurde \ct{curved_android_tastatur_autokorrektur_verbessern}{Abs. 3}, können also den Tippbedarf drastisch verringern.

% -------------------- Überblick -------------------
\subsection{Überblick}
\label{sec:einleitung:ueberblick}

Während berührungsempfindliche Bildschirme schon seit den 1960ern existieren und weiter erforscht werden \ct{newhavendisplay_touchscreen_types_history}{Abs. 6 ff.}, wurden Bildschirmtastaturen erst in den späten 1980ern und 1990ern entwickelt \ctsr{google_patent_us4725694a, google_patent_us4763356a, google_patent_us5276794a, google_patent_us5936614a, google_patent_ca2244431c}. \qt{Infobildschirme[...] im öffentlichen Raum [sollten] [...] stör- und zerstöranfällige Tastaturen vermeiden}\ct{digisaurier_kleine_weltgeschichte_der_tastaturen}{Abs. 10}. Das erste bekannte Beispiel im Konsumentenmarkt ist das IBM Simon aus dem November 1993 -- der Marktstart war im August 1994 \ct{bbc_ibm_simon_celebrates_20_years}{Abs. 2} --, das einen mit einem Eingabestift bedienbaren Bildschirm besaß, der neben E-Mail, Kalender, Dateisystem und Notizen auch die dafür nötige Bildschirmtastatur darbot \ctr{mobilephonemuseum_ibm_simon}.

Richtig populär machte erst Apple 2007 eine Bildschirmtastatur auf dem zur damaligen Zeit noch relativ unbekannten multitouchfähigen Bildschirm \ct{apple_iphone_praesentation_video}{7:12--7:36}, der also mehrere Berührungspunkte erkennen und darauf reagieren konnte, des ersten iPhones. \footref{footnote:apple_iphone_praesentation_video-31:21--31:35}

Obwohl Steve Jobs von der \qt{Erfindung}\ct{apple_iphone_praesentation_video}{7:12--7:16} und \qt{Patentierung}\ct{apple_iphone_praesentation_video}{7:37--7:39} des multitouchfähigen Bildschirms gesprochen hat, sind sowohl die Funktion als auch der Name schon lange vorher entstanden \ct{reshinedisplay_capacitive_touchscreen_knowledge}{Abs. 7--8}. Andere Versuche einer funktionsfähigen Bildschirmtastatur auf solch kleinen Bildschirmen litten jedoch bis zu dem Zeitpunkt meist an Ungenauigkeit und schwerer Bedienung \ctm{\cite[Abs. 2]{fastcompany_ibm_invented_smartphone_then_abandoned}\cite[S. 119, Sp. 2]{inforworld_oct_1994}\cite[Abs. 10]{thoughtco_history_computer_keyboard}}.

In den 18 Jahren seitdem wurden allerdings sehr wenige Fortschritte erzielt \ct{theatlatic_autocorrect_limitations}{Abs. 3}, insbesondere mit Blick auf die Entwicklungsraten anderer Technologien, wie beispielsweise der Prozessorleistung \ct{cpu_benchmark_years}{Graph 1} oder Künstlicher Intelligenz am Beispiel von OpenAIs GPT-Modellen \ctr{datasciencedojo_openai_model_history}:

1. \term{Autokorrektur} wurde erheblich weiterentwickelt und verbessert: Sie ist intelligenter -- sie benutzt statistische Modelle und neuronale Netze für generell bessere und an den Kontext angepasstere Vorschläge \ct{annasleben_sprachverarbeitung}{Abs. 3--4} --, lernfähig -- passt sich über die Zeit dem Benutzer an\ct{annasleben_sprachverarbeitung}{Abs. 5} -- und mehrsprachig \ct{microsoft_swiftkey_unterstuetzte_sprachen}{Abs. 1}.

2. \term{Schreibvorschläge} wurden ebenfalls verbessert -- sie basieren schließlich auf denselben Algorithmen wie die Autokorrektur. Sie können nun neben Worten auch Emojis und bekannte Informationen wie beispielsweise Kontakt- oder Standortdaten vorschlagen \ct{apple_iphone_textvorschlaege}{Abs. 1--2}.

3. Sogenanntes \term{swipe typing} oder \term{swype typing} -- benannt nach der Firma, die es erfunden und schon 2008 veröffentlicht hat \ct{swype_tc50_release_presentation_paper}{Abs. 1} -- war schon früh sehr fehlertolerant und konnte Worte aus sogenannten \emph{Pfaden} formen, die der Benutzer auf das Bild einer Bildschirmtastatur mit dem Finger oder einem Zeiger malte, ohne dabei ein annähernd perfektes Treffen der richtigen Buchstaben zu erfordern \ctr{swype_product}.

Konzeptuell ist es sehr ähnlich zur Autokorrektur, da auch diese mit Wahrscheinlichkeiten arbeitet, um das eigentlich \emph{gemeinte} Wort zu bestimmen, wenn etwas getippt wurde. \ct{theatlatic_autocorrect_limitations}{Abs. 10--11}

In der Anwendung zeichnet es sich vermeintlich vor allem durch seine Schreibgeschwindigkeit aus \footref{footnote:swype_product}, die zwar für den Guinness Weltrekord für das schnellste Tippen einer Textnachricht bekannt war \ctr{techcrunch_swype_guinness_world_record}, es inzwischen aber nicht mehr ist \ctsr{guinnessworldrecords_fastest_text_message_type, guinnessworldrecords_fastest_text_message_swype_type} und nach einer Studie von 2012 auch nie war \ct{sagepub_shape_writing_on_tablets}{Zusammenfassung}.

Da diese Studie allerdings die kontextuelle Komponente des Wortbestimmungsalgorithmus ausgeschaltet hat, lässt sich das Ergebnis zumindest anzweifeln, da dies die Fähigkeiten dieser Art von Tastatur besonders in Grenzfällen einschränkt \ct{grammarly_swipe_typing_keyboard}{Abs. 7--10}.

% ------------------- Zielsetzung ------------------
\subsection{Zielsetzung}
\label{sec:einleitung:zielsetzung}

In der folgenden Arbeit werde ich mich zuerst kurz mit den Grundlagen der Forschung im Bereich der Sprachverarbeitung auseinandersetzen (\autoref{sec:grundlagen_sprachverarbeitung}) und dann zu den verschiedenen Ansätzen für Sprachkorrektur und -vorhersagen in der Gegenwart übergehen (\autoref{sec:gegenwart}): welche Algorithmen benutzt werden, wie sie funktionieren und was ihre jeweiligen Stärken und Schwächen sind.

Anschließend werde ich Herausforderungen bei der Sprachkorrektur und -vorher\-sage aufzeigen (\autoref{sec:gegenwart:herausforderungen}) und mögliche Lösungsansätze für die Zukunft erklären (\autoref{sec:zukunft:moegliche_loesungen}). Dabei wird es sowohl um bereits bestehende, aber noch unausgeprägte als auch theoretisch finalisierte, aber praktisch noch nicht umgesetzte als auch nur theoretisierte Konzepte gehen, deren verschiedene Vor- und Nachteile ich ebenfalls erläutern werde.

Momentane Entwicklungen sowie bestehende Forschungsfragen werde ich kurz anschneiden (\autoref{sec:zukunft:technische_ausblicke}, \ref{sec:zukunft:forschungs_und_praxisfragen}), um dann zu einer Art Fazit beziehungsweise einer Zusammenfassung zu kommen (\autoref{sec:zusammenfassung}).

Zuletzt werde ich einen Ausblick auf meine Facharbeit geben (\autoref{sec:ausblick_auf_die_facharbeit}), über spezifische Ziele theoretisieren, mögliche Methoden zur Evaluation der Wirksamkeit meiner Lösungen darstellen und mich klar von Problemen abgrenzen, deren Lösung über den Rahmen hinausgeht.

% =================== Grundlagen ===================
\section{Grundlage: Sprachverarbeitung}
\label{sec:grundlagen_sprachverarbeitung}

Der Bereich der Sprachverarbeitung oder auch Computerlinguistik kombiniert als Teilgebiet der Informatik und der Künstlichen Intelligenz \ct{ibm_natural_language_processing}{Abs. 1} maschinelles Lernen mit Sprachwissenschaft, um Computern die Fähigkeit zu geben, \qt{menschliche Sprache in geschriebener oder gesprochener Form zu verstehen, zu deuten und zu generieren}\ct{evoluce_sprachverarbeitung}{Abs. 5}.

Er findet vorwiegend Anwendung in der Sentiment-Analyse, bei Chatbots und automatisierten Sprachübersetzungen. \ct{evoluce_sprachverarbeitung}{Abs. 11}

% ---------------- Wichtige Konzepte ---------------
\subsection{Wichtige Konzepte}
\label{sec:grundlagen_sprachverarbeitung:wichtige_konzepte}

Bevor in der Sprachverarbeitung ein Algorithmus mit einem Text etwas anfangen kann, muss dieser zuerst in eine Form gebracht werden, die für Maschinen lesbarer und besser verarbeitbar ist als die für uns Menschen gewöhnliche Aneinanderreihung von Zeichen. Auch Menschen nehmen Buchstaben nicht mehr als einzelne Zeichen wahr, sondern als übergeordnete Buchstabengruppen -- Worte \ct{satzzeichen_wie_gehirn_woerter_erfasst}{Abs. 5}.

Am Beispiel großer Sprachmodelle im Bereich der künstlichen Intelligenz nach heutigem Standard lässt sich die Idee leicht veranschaulichen, auch wenn für jeden Anwendungsfall eine andere Vorverarbeitung benötigt wird:

1. Zuerst wird der Eingabetext nach einem speziellen Muster in Einzelteile aufgespalten; das können Wörter, Wortteile oder sogar Buchstaben genauso wie andere Zeichen sein, die alle durch jeweils eine einzigartige Nummer dargestellt werden. Diesen gesamten Prozess nennt man \term{\usfw{tokenization}}. \ct{microsoft_tokens}{Abs. 1,9,13}

2. Jedem dieser \usfw{token} wird nun ein \term{\usfw{embedding vector}}, ein repräsentativer numerischer Vektor, zugewiesen, der die wahre Bedeutung und den Kontext des \usfw{token} so nuanciert wie möglich erfassen soll. \ctm{\cite[Abs. 1,4]{geeksforgeeks_word_embeddings}\cite[Abs. 21]{microsoft_tokens}}

3. Im Falle der Transformer-Modelle -- eine spezielle Form der großen Sprachmodelle, der alle großen Chatbots heute entsprechen \ct{abzglobal_ai_chatbots_history}{Abs. 16} -- werden die Vektoren nun durch eine Reihe sogenannter \term{\usfw{attention layers}} geschickt, Funktionen, die den gesamten Kontext in einer bisher einzigartigen Art und Weise mit den \usfw{embedding vectors} verflechten, die die Relevanz von Wörtern berücksichtigt. \ct{geeksforgeeks_transformer_attention_mechanism}{Abs. 1--2}

Das Ergebnis dieser Schritte \usfw{enthält} - wenn auch nicht für Menschen verständlich -- fast alle Informationen, die es zum Eingabetext geben kann -- inklusive der Verbesserungen von Unklarheiten und Fehlern --, nur in sehr kondensierter Form. Vereinfacht gesagt können große Sprachmodelle hiervon direkt ihre Antwort ableiten. \ct{patrickstolp_transformer}{Abs. 22--23}

Auch Sprachkorrektur und -vorhersagen nutzen diesen Prozess, in der Regel ist ein Teil ihrer Funktionsweise sogar -- abgesehen von seiner Größe -- in seiner algorithmischen Grundstruktur genau wie bei großen Sprachmodellen. Schließlich ist, das nächste Wort vorherzusagen, wie es die Sprachvorhersage tut, genau dasselbe, was diese Modelle machen, nur mit \emph{einem} nächsten Wort anstatt einer vollständigen Antwort. \ct{datacamp_small_language_models}{Abs. 1,11,13,15,30}

% ==================== Gegenwart ===================
\section{Gegenwart}
\label{sec:gegenwart}

% ------------- Algorithmen und Modelle ------------
\subsection{Algorithmen und Modelle}
\label{sec:gegenwart:algorithmen_und_modelle}

Sprachkorrektur und -vorhersagen sind heute stark von auf künstlicher Intelligenz basierenden Methoden geprägt, die vor allem das Kontextverständnis enorm verbessern \ct{evoluce_fehlerkorrektur}{Abs. 7,12,15}. Trotzdem wird sich bei den heutigen Bildschirmtastaturen nicht nur auf dieses nur wahrscheinlich richtige\ctm{\cite[Abs. 2,9--10]{lernenwiemaschinen_wahrscheinlichkeit_ki}\cite[Abs. 2--3]{tagesschau_ki_erfindet_jede_dritte_antwort}}, aber auch schwer nachvollziehbare\ct{fraunhofer_ki_blackbox}{Abs. 1--2} Mittel verlassen, um den Benutzer so unauffällig und -dringlich wie möglich beim Tippen in jeglichem Umfeld zu unterstützen \ct{languagetool_kuenstliche_intelligenz_bessere_korrektur}{Abs. 14--15}; ganz ohne probabilistische Verfahren kommen moderne Bildschirmtastaturen allerdings schwer aus: Sogar Tastaturen für Fernseher, die man nur mit Pfeiltasten kontrolliert, können optimiert werden \ctr{maxhalfordgithub_dynamic_onscreen_tv_keyboards}.

\subsubsection{Statistische Sprachmodelle}
\label{sec:gegenwart:algorithmen_und_modelle:statistische_sprachmodelle}

Algorithmen dieser Kategorie bauen regelmäßig auf einer großen Datenbasis auf, um verlässlich funktionieren zu können \ct{degruyterbrill_statistisch_basierte_sprachmodelle}{Abs. 1}. Sie analysieren den gegebenen Datensatz und können zum Beispiel die Häufigkeiten, mit denen bestimmte Wortfolgen oder Muster auftreten, erkennen \ct{baeldung_ngram}{Abs. 22}.

Der einfachste Ansatz für ein statistisches Modell ist das \term{N-Gramm}: Hierbei werden die Wahrscheinlichkeiten aller -- im Datensatz enthaltenen -- Wortfolgen von $N$ Worten festgestellt, sodass bei einer vorangegangenen Anzahl von $N-1$ Wörtern das nächste Wort, das im Datensatz am häufigsten nach diesen Wörtern stand, mit einer bestimmten Wahrscheinlichkeit vorhergesagt werden kann. \ctm{\cite[Folie 20--25]{tuchemnitz_sprachmodellierung}\cite[Abs. 5, Punkt 1]{computerweekly_sprachmodellierung}}

Dabei kommen folgende Ausprägungen am meisten zur Anwendung \ct{baeldung_ngram}{Abs. 19}:

1. Unigramme (1-Gramme) -- wie häufig kommt ein einziges Wort vor -- \ctm{\cite[Folie 22]{tuchemnitz_sprachmodellierung}\cite[Abs. 3, Punkt 1]{medium_ngrams_in_nlp}},

2. Bigramme (2-Gramme) -- wie häufig folgt ein Wort auf ein anderes -- \ctm{\cite[Folie 24--25]{tuchemnitz_sprachmodellierung}\cite[Abs. 3, Punkt 2]{medium_ngrams_in_nlp}} und 

3. Trigramme (3-Gramme) bis 5-Gramme -- wie oft folgt auf eine Kette von Wörtern ein anderes \ctm{\cite[Folie 28]{tuchemnitz_sprachmodellierung}\cite[Abs. 3, Punkt 3--4]{medium_ngrams_in_nlp}}.

Längere Ketten von Wörtern führen zu exponentiell mehr Kombinationsmöglichkeiten, erfordern ebensoviel mehr Trainingsdaten und bringen außerdem durch das N-Grammen generell fehlende Kontextverständnis keine nennenswerten Vorteile mehr \ct{dataleap_geschichte_sprachmodellierung}{Abs. 2,8--9}.

Es gibt viele Abwandlungen, die verschiedene Schwächen von N-Grammen korrigieren sollen, wie beispielsweise bidirektionale oder exponentielle N-Gramme, die Häufigkeiten von sowohl folgenden als auch vorangehenden Wörtern zählen \ct{computerweekly_sprachmodellierung}{Abs. 5, Punkt 2} beziehungsweise die zusätzliche Mechanismen in die Wahrscheinlichkeitsrechnung einbauen, die den Kontext mehr berücksichtigen und zu schnelle Rückschlüsse aus den Daten verhindern sollen \ctm{\cite[Abs. 5, Punkt 2]{computerweekly_sprachmodellierung}\cite[Folie 10]{oxford_maximum_entropy_modelling}}.

Ein weiterer sehr populärer aber deutlich komplexerer Ansatz ist das \term{neuronale Netz}: Vereinfacht gesagt werden die Parameter einer Reihung von Matrixmultiplikationen -- den Neuronen und Synapsen in unserem Gehirn nachgebildet \ctm{\cite[Abs. 2]{pangeanic_neuronale_netze}\cite[Folie 16--21]{hsmannheim_grundlagen_neuronale_netze}} -- mit einer idealerweise extrem großen Menge an Trainingsdaten so lange angepasst, bis aus einer kodierten Wortfolge die Wahrscheinlichkeitsverteilung des darauf folgenden Wortes \emph{errechnet} werden kann. \ctm{\cite[Abs. 2--4,Formel 3]{cornellbowers_neural_networks_matrix_multiply}\cite[Abs. 3--7]{pangeanic_neuronale_netze}}

Diesem Algorithmus sind \emph{in der Theorie} keine Grenzen gesetzt, sowohl für die Länge der Wortfolge als auch für die Anzahl der Parameter als auch für die Genauigkeit der Ausgabe \ctm{\cite[Abs. 1]{liquidnews_ki_grenzenlos}\cite[Abs. 5]{visusadvisory_starke_schwache_ki}\cite[Abs. 1--3]{ibm_artificial_general_intelligence}}. \emph{In der Praxis} erreichen Modelle allerdings nie perfekte Quoten \ctm{\cite[Abs. 1--6]{medium_escaping_trap_local_minima}\cite[Abs. 2--6]{futurism_openai_losing_money_on_chatgpt}}, da ein gewisses finanzielles Interesse Unternehmen davon abhält, häufig kostenlos oder unbegrenzt Benutzern zur Verfügung gestellten\ctm{\cite[\qt{Free},\qt{Pro} Abonnement]{openai_chatgpt_pricing}\cite[\qt{Paraphrasierung}]{languagetool_premium}} Modellen auch nur annähernd genug Rechenkapazität zu geben.

Bevor die größten Modelle in ihrer Größe jedoch weiter wachsen, fehlt es als Erstes an einer ganz anderen Ressource: Trainingsdaten, besonders hochqualitative \ctm{\cite[Abs. 1--2,28--29]{medium_solution_insufficient_llm_training_data}\cite[Abs. 1--2,37,51]{linkedin_chatgpt_consumed_internet}}.

Trotzdem sind neuronale Netze älteren Modellen wie N-Grammen weit überlegen, weil sie Kontext in einer Menge und Art berücksichtigen können, die mit N-Grammen unrealistisch zu erreichen ist \ctm{\cite[Folie 46--49]{tuchemnitz_sprachmodellierung}\cite[Abs. 1,29]{towardsdatascience_neural_networks_over_ngram}}, selbst wenn theoretisch ein solches System mithilfe zusätzlicher Regeln erschaffen werden könnte \ct{geeksforgeeks_rule_based_nlp}{Abs. 1,4--5,8,10}.

\subsubsection{Wissensbasierte Sprachmodelle}
\label{sec:gegenwart:algorithmen_und_modelle:wissensbasierte_sprachmodelle}

Algorithmen aus dieser Kategorie verlassen sich regelmäßig auf handgefertigte Regelsätze \ct{?}{?}. Ein prominentes Beispiel ist hier LanguageTool, ein Rechtschreib- und Grammatikkorrektur-Programm, das sich neben einer neueren Künstlichen Intelligenz vor allem auf einen großen Regelkorpus stützt \ct{?}{?}.

Regelbasierte Methoden sind heute allerdings nicht mehr wirklich relevant \ct{?}{?}. Es gibt zu viele Sprachen, zu viele Regeln und vor allem zu viele Sonderfälle \ct{?}{?}. Wissensbasierte Sprachmodelle können in der Theorie perfekte oder zumindest die besten Voraussagen leisten, sind jedoch in der Praxis sowohl im Aufbau als auch in der Instandhaltung und Pflege schlicht zu aufwändig \ct{?}{?}.

% ---------------- Herausforderungen ---------------
\subsection{Herausforderungen}
\label{sec:gegenwart:herausforderungen}

Auf jeden Fall steht fest, dass die Sprachkorrektur früher besser als die Sprachvorhersage sein muss, weil Fehler dort eine schlimmere Auswirkung haben: Vorschläge gibt es mehrere und sie sind optional, heißt, der Benutzer kann immer noch entscheiden, sie nicht auszuwählen. Autokorrektur hingegen arbeitet im Hintergrund und kann entweder alles überprüfen und möglicherweise korrigieren oder gar nichts.

Die drei großen Herausforderungen, vor denen die Entwickler der Sprachkorrektur stehen, sind:

1. \label{par:herausforderungen:1_kontext}\term{Kontext}: Der Algorithmus muss sowohl den Satz, den Paragrafen und den gesamten Text als auch den Softwarekontext kennen. Während Ersteres zwar grundsätzlich schon durch große neuronale Netze mehr oder weniger erfolgreich angegangen wird \ct{?}{?}, braucht es für Letzteres erst einmal\ct{?}{?} eine genormte, allgemein akzeptierte und implementierte technische Grundlage. Dabei wird so gut wie jede Person zum Beispiel an verschiedene andere Personen auf verschiedene Arten Nachrichten schreiben und wieder anders digitales Tagebuch führen.

2. \term{Mischsprache}: Viele Nutzer berichten in den sozialen Medien, dass sie selbst oder Bekannte von ihnen häufig mehrere Sprachen in einem Text vereinen \ct{?}{?}. Das stellt den Korrekturalgorithmus vor eine besonders komplizierte Aufgabe, denn Wörterbücher verschiedener Sprachen lassen sich sicher vereinen \ct{?}{?}, Grammatik und Kontext sind allerdings deutlich schwieriger zu kombinieren \ct{?}{?}.

3. \term{Schreibstil}: Individuelle Wörter, Abkürzungen oder Wendungen, die nicht im gewöhnlichen oder durchschnittlichen Sprachgebrauch in der Form oder Häufigkeit vorkommen, sollen zwar durch lernfähige Methoden berücksichtigt werden \ct{?}{?}, das muss aber nicht immer so funktionieren; dabei kann sowohl zu wenig als auch zu viel oder aber nicht in den richtigen Momenten Rücksicht genommen werden \ct{?}{?}.

Alle Probleme für alle Nutzer werden sich wohl nie lösen lassen \ct{?}{?}, was, genau genommen, ein weiteres Problem darstellt: Bei der Entwicklung des Algorithmus bis jetzt und der Weiterentwicklung in der Zukunft wird stets auf eine möglichst große Masse gezielt werden, an deren Schreibverhalten sie sich anpassen \ct{?}{?}. Minderheiten würden dadurch stets außen vor gelassen.

Die letzte Herausforderung liegt in dem Wechsel von externer Berechnung, der Abhängigkeit von der Cloud, hin zur lokalen Berechnung \ct{?}{?}. Das hat verschiedene Vorteile, beispielsweise mehr Privatsphäre und schnellere Ergebnisse \ct{?}{?}, gleichzeitig aber auch Nachteile: Große Sprachmodelle, wie sie heute benutzt werden, können technisch bedingt nicht auf schwachen Konsumentenmobilgeräten laufen \ct{?}{?}; das Verkleinern verringert die Genauigkeit der Ergebnisse \ct{?}{?}.

% ===================== Zukunft ====================
\section{Zukunft}
\label{sec:zukunft}

% ---------------- Mögliche Lösungen ---------------
\subsection{Mögliche Lösungen}
\label{sec:zukunft:moegliche_loesungen}

\subsubsection{1. Kontext}
\label{sec:zukunft:moegliche_loesungen:1_kontext}

\textit{Ideen: auf technische Grundlage drängen/diese schaffen, bessere/diversere Trainingsdaten, mehr regelbasierte Ansätze}

\subsubsection{2. Mischsprache}
\label{sec:zukunft:moegliche_loesungen:2_mischsprache}

\textit{Ideen: mehr Trainingsdaten, Analyse von Gebrauch => gezielte Implementierung}

\subsubsection{3. Schreibstil}
\label{sec:zukunft:moegliche_loesungen:3_schreibstil}

\textit{Ideen: separate, auswählbare Algorithmen entwickeln, nicht einen generellen für alle, weitgehende Studie zur Analyse verschiedener Schreibstile}

\subsubsection{4. Minderheiten}
\label{sec:zukunft:moegliche_loesungen:4_minderheiten}

\textit{Ideen: (s. Punkt 3)}

\subsubsection{5. Lokalisierung}
\label{sec:zukunft:moegliche_loesungen:5_lokalisierung}

\textit{Ideen: Forschung und Verbesserung von SLMs, mehr regelbasierte Ansätze, Verbesserung der Hardware (KI-Chips für Gemini sind sowieso schon in Handys)}

% -------------- Technische Ausblicke --------------
\subsection{Technische Ausblicke}
\label{sec:zukunft:technische_ausblicke}

\textit{Hardware auch auf Mobilgeräten immer mehr für KI geeignet, SLMs zunehmend realisierbarer, bessere Trainingsdaten(?)}

% ---------- Forschungs- und Praxisfragen ----------
\subsection{Forschungs- und Praxisfragen}
\label{sec:zukunft:forschungs_und_praxisfragen}



% ================= Zusammenfassung ================
\section{Zusammenfassung}
\label{sec:zusammenfassung}



% =========== Ausblick auf die Facharbeit ==========
\section{Ausblick auf die Facharbeit}
\label{sec:ausblick_auf_die_facharbeit}



% --- Problemstellungen und mögliche Zielsetzung ---
\subsection{Problemstellungen und mögliche Zielsetzung}
\label{sec:ausblick_auf_die_facharbeit:problemstellungen_und_moegliche_zielsetzungen}



% ------------ Evaluation und Bewertung ------------
\subsection{Evaluation und Bewertung}
\label{sec:ausblick_auf_die_facharbeit:evaluation_und_bewertung}



% ---------------- Problemabgrenzung ---------------
\subsection{Problemabgrenzung}
\label{sec:ausblick_auf_die_facharbeit:problemabgrenzung}



\pagebreak
% ############## Literaturverzeichnis ##############
\bibliographystyle{alphadin}
\bibliography{bibliografie}

\end{document}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%