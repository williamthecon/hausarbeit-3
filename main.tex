% ###################### Form ######################
\documentclass{article}

\usepackage{geometry}
\geometry{
 a4paper,
 total={130mm,257mm},
 left=30mm,
 top=20mm
}

\usepackage{setspace}
\onehalfspacing

\usepackage[utf8]{inputenc}

\usepackage[T1]{fontenc}
\usepackage{lmodern}

% ###################### Stil ######################
\usepackage[ngerman]{babel}
\usepackage{graphicx}

\usepackage{xurl} % korrekte Umbrüche in Hyperlinks
\usepackage[
    colorlinks=true, % verhindert Linkbox, aber färbt
    urlcolor=black, % deshalb alles schwarz
    linkcolor=black,
    citecolor=black,
    filecolor=black,
    breaklinks=true
]{hyperref} % korrekter Hyperlink, sogar bei Umbrüchen

\usepackage{dblfnote} % Footnotes in zwei Spalten
\usepackage{footmisc} % Footnotes referieren

\DeclareUnicodeCharacter{2248}{\ensuremath{\approx}} % ungefähr gleich außerhalb von Mathe

% ##################### Befehle ####################
\newcommand{\term}[1]{\textbf{#1}} % term
\newcommand{\usfw}[1]{\textit{#1}} % umgangsprachlich/fremdwort

% These cite commands make for an easier switch between in-text and footnote citations
\newcommand{\ct}[2]{\footnote{\label{footnote:#1-#2}\cite[#2]{#1}}} % cite (normal: source, location)
\newcommand{\ctr}[1]{\footnote{\label{footnote:#1}\cite{#1}}} % cite ("raw": only source)
\usepackage{xparse}
% \NewDocumentCommand{\cts}{o m}{ % cite (multiple sources)
%   \IfNoValueTF{#1}
%     {\footnote{\cite{#2}}}%
%     {\footnote{\cite[#1]{#2}}}%
% }
\NewDocumentCommand{\ctsr}{m}{\footnote{\cite{#1}}} % cite (multiple sources raw)
\newcommand{\ctm}[1]{\footnote{#1}} % cite manual [only footnote, `\cite` needs to be used]

\newcommand{\qt}[1]{\glqq{}#1\grqq} % quote

% #################### Metadaten ###################
\title{Untersuchung der Algorithmen zur Sprachkorrektur und -vorhersage am Beispiel der Bildschirmtastatur}
\author{jafri}
\date{Oktober -- November 2025}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

% ################### Titelblatt ###################
%\maketitle
\begin{titlepage}\centering

  {\Large Gymnasium At}\\[1.5cm]
  
  {\huge Hausarbeit}\\[0.5cm]
  {\LARGE Thema: Untersuchung der Algorithmen zur Sprachkorrektur und -vorhersage am Beispiel der Bildschirmtastatur}\\[2cm]
  
  {\large Vorgelegt von}\\[0.3cm]
  {\Large jafri}\\[0.3cm]
  {\large Jahrgang 12}\\[1cm]
  
  {\large Seminarfach: SIA}\\[0.3cm]
  {\large betreut von Herrn SWA}\\[1.5cm]
  
  {\large Ort, Oktober -- November 2025}

\end{titlepage}

\pagebreak
% ############### Inhaltsverzeichnis ###############
\tableofcontents

\pagebreak
% ##################### Inhalt #####################

% =================== Anmerkungen ==================
\section*{Hinweise}
\addcontentsline{toc}{section}{Hinweise}

\subsection*{Sprache}

Im Folgenden sind jegliche maskulinen, femininen oder neutralen Wortformen, sofern der Kontext es zulässt und nicht explizit das Gegenteil angegeben ist, als generische Formen zu verstehen. Der Einfach- und Knappheit halber wird auf aufwändigere und in der Regel längere geschlechtsneutrale Formen verzichtet, auch mangels einer standardisierten Schreibweise.

\subsection*{Zitieren}

Fußnoten folgen festen Regeln, um ihren Bezug klarzumachen:

\begin{itemize}
    \item Stehen sie nach einem Satzzeichen, beziehen sie sich mindestens auf den eben beendeten Haupt-, Neben- oder Ganzsatz.
    \item Befinden sie sich gleichzeitig am Paragrafenende, beziehen sie sich regelmäßig auf den gesamten Paragrafen, sofern es sich bei allen Sätzen um zu belegende und noch nicht belegte Aussagen mit sehr verwandtem Thema handelt.
    \item Folgen sie direkt einer Wortfolge, deren Buchstaben \textsl{leicht schräg gestellt} sind, beziehen sie sich ausschließlich auf diese.
    \item Kommt direkt vor ihnen ein normales Wort, so beziehen sie sich schlicht nur auf dieses Wort.
\end{itemize}
    
Um Absätze (\textit{Abs.}) handelt es sich in Quellen, wenn sie Paragrafen von Text, einem oder mehreren nummerierten oder nicht nummerierten Stichpunkten im Textzusammenhang bezeichnen, die meist in der in dem speziellen Text üblichen Art und Weise -- vor allem im Abstand -- vom Rest des Textes abgegrenzt sind. Ausgeschlossen sind also zum Beispiel Tabellen, Bildunterschriften oder Verweise auf andere Artikel, Zusammenfassungen vor dem eigentlichen Textbeginn werden genauso nicht mitgezählt. Auf andere Teile des Textes kann explizit hingewiesen werden.

Jegliche Quellen stehen in digitaler Form über die angegebenen Internetadressen zur Verfügung. Abweichungen durch Änderungen nach dem ebenfalls angegebenen Abrufdatum lassen sich durch die in der Bibliografie hinzugefügten \textit{archive}-Attribute -- Internetarchiv-Adressen oder Hinweise auf eine lokale Speicherung -- nachvollziehen und werden im Folgenden nicht berücksichtigt.

% =================== Einleitung ===================
\section{Einleitung}
\label{sec:einleitung}

Beim Tippen auf Bildschirmtastaturen, wie sie auf modernen Mobiltelefonen oder Tabletcomputern mit berührungsempfindlichem Bildschirm zu finden sind, bleiben viele im Hintergrund ablaufende Prozesse für den Nutzer unsichtbar. Idealerweise ist die Benutzererfahrung mit einer digitalen Tastatur so problemlos, dass Korrekturmaßnahmen und Optimierungen nicht auffallen, die Nachteile der geringeren Größe im Vergleich zur physischen Tastatur ausgleichen sollen.\ct{iphoneticker_iphone_entwicklung}{Abs. 5--6}

In der Realität funktioniert das meist nicht so gut, wie zahlreiche Nutzerberichte und Diskussionen in sozialen Medien zeigen. Hier wird vor allem die vermeintliche Korrektur von bereits richtig geschriebenen Wörtern kritisiert.\ctsr{reddit_autokorrect_schlecht_1,reddit_autokorrect_schlecht_2}

\term{Autokorrektur}, wie der Algorithmus genannt wird, der Tippfehler korrigieren soll, ist schon für die erste wirklich erfolgreiche Bildschirmtastatur implementiert -- Tippfehler sind fast unvermeidbar.\ct{apple_iphone_praesentation_video}{31:21--31:35}

\term{Schreibvorschläge}, vorwiegend in einer Leiste über der eigentlichen Tastatur, sind ergänzend dazu die Norm. Sie geben dem Benutzer während des Tippens auswählbare Vorschläge, zu denen das momentan direkt vor, um oder nach dem Cursor befindliche Wort geändert werden kann, aber nicht muss.\ctm{\cite[Abs. 1,2]{techbone_android_vorschlaege_verwenden_einstellen}\cite[Abs. 2,6--7]{apple_autokorrektur_vorschlaege_verwenden}}

Sie sind besonders dann nützlich, wenn ein falsch geschriebenes Wort gleich ähnlich zu verschiedenen Worten ist, die auch alle kontextuell möglich und gleich wahrscheinlich sind: Autokorrektur müsste in dem Fall ein zufälliges Wort auswählen; der Nutzer weiß besser, was er tippen will.

Außerdem sind sie schon verfügbar, bevor überhaupt ein Buchstabe des nächsten Wortes getippt wurde,\ct{curved_android_tastatur_autokorrektur_verbessern}{Abs. 3} können also den Tippbedarf theoretisch drastisch verringern. Die Praxis hat jedoch ergeben, dass es meist länger dauert, den Vorschlag auszuwählen, als das Wort zu Ende zu tippen.\ct{ethzuerich_smartphone_typing_speeds}{Abs. 2}

% -------------------- Überblick -------------------
\subsection{Überblick}
\label{sec:einleitung:ueberblick}

Während berührungsempfindliche Bildschirme schon seit den 1960ern existieren und weiter erforscht werden,\ct{newhavendisplay_touchscreen_types_history}{Abs. 6 ff.} wurden Bildschirmtastaturen erst in den späten 1980ern und 1990ern entwickelt.\ctsr{google_patent_us4725694a, google_patent_us4763356a, google_patent_us5276794a, google_patent_us5936614a, google_patent_ca2244431c} \qt{Infobildschirme[\ldots] im öffentlichen Raum [sollten] [\ldots] stör- und zerstöranfällige Tastaturen vermeiden}\ct{digisaurier_kleine_weltgeschichte_der_tastaturen}{Abs. 10}. Das erste bekannte Beispiel im Konsumentenmarkt ist das IBM Simon aus dem November 1993 -- der Marktstart war im August 1994 --\ct{bbc_ibm_simon_celebrates_20_years}{Abs. 2}, das einen mit einem Eingabestift bedienbaren Bildschirm besaß, der neben E-Mail, Kalender, Dateisystem und Notizen auch die dafür nötige Bildschirmtastatur darbot.\ctr{mobilephonemuseum_ibm_simon}

Richtig populär machte erst Apple 2007 eine Bildschirmtastatur auf dem \textsl{zur damaligen Zeit noch relativ unbekannten multitouchfähigen Bildschirm}\ct{apple_iphone_praesentation_video}{7:12--7:36} des ersten iPhones.\footref{footnote:apple_iphone_praesentation_video-31:21--31:35}

Obwohl Steve Jobs von der \qt{Erfindung}\ct{apple_iphone_praesentation_video}{7:12--7:16} und \qt{Patentierung}\ct{apple_iphone_praesentation_video}{7:37--7:39} des multitouchfähigen Bildschirms gesprochen hat, sind sowohl die Funktion als auch der Name schon lange vorher entstanden.\ct{reshinedisplay_capacitive_touchscreen_knowledge}{Abs. 7--8} Andere Versuche einer funktionsfähigen Bildschirmtastatur auf solch kleinen Bildschirmen litten jedoch bis zu dem Zeitpunkt meist an Ungenauigkeit und schwerer Bedienung.\ctm{\cite[Abs. 2]{fastcompany_ibm_invented_smartphone_then_abandoned}\cite[S. 119, Sp. 2]{inforworld_oct_1994}\cite[Abs. 10]{thoughtco_history_computer_keyboard}}

In den 18 Jahren seitdem wurden allerdings sehr wenige Fortschritte erzielt,\ct{theatlatic_autocorrect_limitations}{Abs. 3} insbesondere mit Blick auf die Entwicklungsraten anderer Technologien, wie beispielsweise der Prozessorleistung\ct{cpu_benchmark_years}{Graph 1} oder Künstlicher Intelligenz am Beispiel von \textsl{OpenAIs GPT-Modellen}\ctr{datasciencedojo_openai_model_history}:

\begin{enumerate}

    \item \term{Autokorrektur} wurde erheblich weiterentwickelt und verbessert: Sie ist intelligenter -- sie benutzt statistische Modelle und neuronale Netze für generell bessere und an den Kontext angepasstere Vorschläge --\ct{annasleben_sprachverarbeitung}{Abs. 3--4}, lernfähig -- passt sich über die Zeit dem Benutzer an --\ct{annasleben_sprachverarbeitung}{Abs. 5} und mehrsprachig\ct{microsoft_swiftkey_unterstuetzte_sprachen}{Abs. 1}.
    
    \item \term{Schreibvorschläge} wurden ebenfalls verbessert -- sie basieren schließlich auf denselben Algorithmen wie die Autokorrektur. Sie können nun neben Worten auch Emojis und bekannte Informationen wie beispielsweise Kontakt- oder Standortdaten vorschlagen.\ct{apple_iphone_textvorschlaege}{Abs. 1--2}
    
    \item Sogenanntes \term{swipe typing} oder \term{swype typing} -- benannt nach der Firma, die es erfunden und schon 2008 veröffentlicht hat --\ct{swype_tc50_release_presentation_paper}{Abs. 1} war schon früh sehr fehlertolerant und konnte Worte aus sogenannten Pfaden formen, die der Benutzer auf das Bild einer Bildschirmtastatur mit dem Finger oder einem Zeiger malte, ohne dabei ein annähernd perfektes Treffen der richtigen Buchstaben zu erfordern.\ctr{swype_product}

    Konzeptuell ist es sehr ähnlich zur Autokorrektur, da auch diese mit Wahrscheinlichkeiten arbeitet, um das eigentlich \emph{gemeinte} Wort zu bestimmen, wenn etwas getippt wurde.\ct{theatlatic_autocorrect_limitations}{Abs. 10--11}

    In der Anwendung zeichnet es sich vermeintlich vor allem durch seine Schreibgeschwindigkeit aus,\footref{footnote:swype_product} die zwar für den Guinness-Weltrekord für das schnellste Tippen einer Textnachricht bekannt war,\ctr{techcrunch_swype_guinness_world_record} es \textsl{inzwischen aber nicht mehr ist}\ctsr{guinnessworldrecords_fastest_text_message_type, guinnessworldrecords_fastest_text_message_swype_type} und \textsl{nach einer Studie von 2012 auch nie war}\ct{sagepub_shape_writing_on_tablets}{Zusammenfassung}.

    Da diese Studie allerdings die kontextuelle Komponente des Wortbestimmungsalgorithmus ausgeschaltet hat, lässt sich das Ergebnis zumindest anzweifeln, da dies die Fähigkeiten dieser Art von Tastatur besonders in Grenzfällen einschränkt.\ct{grammarly_swipe_typing_keyboard}{Abs. 7--10}

\end{enumerate}

% ------------------- Zielsetzung ------------------
\subsection{Zielsetzung}
\label{sec:einleitung:zielsetzung}

In der folgenden Arbeit werde ich mich zuerst kurz mit den Grundlagen der Forschung im Bereich der Sprachverarbeitung auseinandersetzen (\autoref{sec:grundlagen_sprachverarbeitung}) und dann zu den verschiedenen Ansätzen für Sprachkorrektur und -vorhersagen in der Gegenwart übergehen (\autoref{sec:gegenwart}): welche Algorithmen benutzt werden, wie sie funktionieren und was ihre jeweiligen Stärken und Schwächen sind.

Anschließend werde ich Herausforderungen bei der Sprachkorrektur und -vorher\-sage aufzeigen (\autoref{sec:gegenwart:herausforderungen}) und mögliche Lösungsansätze für die Zukunft erklären (\autoref{sec:zukunft_und_moegliche_loesungen}). Dabei wird es sowohl um bereits bestehende, aber noch unausgeprägte als auch theoretisch finalisierte, aber praktisch noch nicht umgesetzte als auch nur theoretisierte Konzepte gehen, deren verschiedene Vor- und Nachteile ich ebenfalls erläutern werde.

Schließlich werde ich zu einer Art Fazit beziehungsweise einer Zusammenfassung kommen (\autoref{sec:zusammenfassung}), die von einem Ausblick auf meine Facharbeit gefolgt ist (\autoref{sec:ausblick_auf_die_facharbeit}), in dem ich über spezifische Ziele theoretisieren, mögliche Methoden zur Evaluation der Wirksamkeit meiner Lösungen darstellen und mich klar von Problemen abgrenzen werde, deren Lösung über den Rahmen hinausgeht.

% =================== Grundlagen ===================
\section{Grundlage: Sprachverarbeitung}
\label{sec:grundlagen_sprachverarbeitung}

Der Bereich der Sprachverarbeitung oder auch Computerlinguistik kombiniert als \textsl{Teilgebiet der Informatik und der Künstlichen Intelligenz}\ct{ibm_natural_language_processing}{Abs. 1} maschinelles Lernen mit Sprachwissenschaft, um Computern die Fähigkeit zu geben, \qt{menschliche Sprache in geschriebener oder gesprochener Form zu verstehen, zu deuten und zu generieren}\ct{evoluce_sprachverarbeitung}{Abs. 5}.

Er findet vorwiegend Anwendung in der Sentiment-Analyse, bei Chatbots und automatisierten Sprachübersetzungen.\ct{evoluce_sprachverarbeitung}{Abs. 11}

\begin{center}***\end{center}

Bevor in der Sprachverarbeitung ein Algorithmus mit einem Text arbeiten kann, muss dieser zuerst in eine Form gebracht werden, die für Maschinen lesbarer und besser verarbeitbar ist als die für uns Menschen gewöhnliche Aneinanderreihung von Zeichen. Auch Menschen nehmen Buchstaben nicht mehr als einzelne Zeichen wahr, sondern als übergeordnete Buchstabengruppen -- Worte.\ct{satzzeichen_wie_gehirn_woerter_erfasst}{Abs. 5}

Am Beispiel großer Sprachmodelle im Bereich der Künstlichen Intelligenz nach heutigem Standard lässt sich die Idee leicht veranschaulichen, auch wenn für jeden Anwendungsfall eine andere Vorverarbeitung benötigt wird:

\begin{enumerate}

    \item Zuerst wird der Eingabetext nach einem speziellen Muster in Einzelteile aufgespalten; das können Wörter, Wortteile oder sogar Buchstaben genauso wie andere Zeichen sein, die alle durch jeweils eine einzigartige Nummer dargestellt werden. Diesen gesamten Prozess nennt man \term{\usfw{tokenization}}.\ct{microsoft_tokens}{Abs. 1,9,13}

    \item Jedem dieser \usfw{token} wird jetzt ein \term{\usfw{embedding vector}}, ein repräsentativer numerischer Vektor, zugewiesen, der die wahre Bedeutung und den Kontext des \usfw{token} so nuanciert wie möglich erfassen soll.\ctm{\cite[Abs. 1,4]{geeksforgeeks_word_embeddings}\cite[Abs. 21]{microsoft_tokens}}

    \item Im Falle der Transformer-Modelle -- eine spezielle Form der großen Sprachmodelle, der alle großen Chatbots heute entsprechen --\ct{abzglobal_ai_chatbots_history}{Abs. 16} werden die Vektoren nun durch eine Reihe sogenannter \term{\usfw{attention layers}} geschickt, Funktionen, die den gesamten Kontext in einer bisher einzigartigen Art und Weise mit den \usfw{embedding vectors} verflechten, die die Relevanz von Wörtern berücksichtigt.\ct{geeksforgeeks_transformer_attention_mechanism}{Abs. 1--2}

\end{enumerate}

Das Ergebnis dieser Schritte \usfw{enthält} - wenn auch nicht für Menschen verständlich -- fast alle Informationen, die es zum Eingabetext geben kann -- inklusive der Verbesserungen von Unklarheiten und Fehlern --, nur in sehr kondensierter Form. Vereinfacht gesagt können große Sprachmodelle hiervon direkt ihre Antwort ableiten.\ct{patrickstolp_transformer}{Abs. 22--23}

Auch Sprachkorrektur und -vorhersagen nutzen diesen Prozess, in der Regel ist ein Teil ihrer Funktionsweise sogar -- abgesehen von der Größe -- in der algorithmischen Grundstruktur genau wie große Sprachmodelle. Schließlich ist, das nächste Wort vorherzusagen, wie es die Sprachvorhersage tut, genau dasselbe, was diese Modelle machen, nur mit \emph{einem} nächsten Wort anstatt einer vollständigen Antwort.\ct{datacamp_small_language_models}{Abs. 1,11,13,15-18,31--33}

% ==================== Gegenwart ===================
\section{Gegenwart}
\label{sec:gegenwart}

% ------------- Algorithmen und Modelle ------------
\subsection{Algorithmen und Modelle}
\label{sec:gegenwart:algorithmen_und_modelle}

Sprachkorrektur und -vorhersagen sind heute stark von auf Künstlicher Intelligenz basierenden Methoden geprägt, die vor allem das Kontextverständnis enorm verbessern.\ct{evoluce_fehlerkorrektur}{Abs. 7,12,15} Trotzdem wird sich bei den heutigen Bildschirmtastaturen nicht nur auf dieses \textsl{nur wahrscheinlich richtige}\ctm{\cite[Abs. 2,9--10]{lernenwiemaschinen_wahrscheinlichkeit_ki}\cite[Abs. 2--3]{tagesschau_ki_erfindet_jede_dritte_antwort}}, aber auch \textsl{schwer nachvollziehbare}\ct{fraunhofer_ki_blackbox}{Abs. 1--2} Mittel verlassen, um den Benutzer so unauffällig und -dringlich wie möglich beim Tippen in jeglichem Umfeld zu unterstützen;\ct{languagetool_kuenstliche_intelligenz_bessere_korrektur}{Abs. 14--15} ganz ohne probabilistische Verfahren kommen moderne Bildschirmtastaturen allerdings schwer aus: Sogar Tastaturen für Fernseher, die man nur mit Pfeiltasten kontrolliert, können optimiert werden.\ctr{maxhalfordgithub_dynamic_onscreen_tv_keyboards}

\subsubsection{Statistische Sprachmodelle}
\label{sec:gegenwart:algorithmen_und_modelle:statistische_sprachmodelle}

Algorithmen dieser Kategorie bauen regelmäßig auf einer großen Datenbasis auf, um verlässlich funktionieren zu können.\ct{degruyterbrill_statistisch_basierte_sprachmodelle}{Abs. 1} Sie analysieren den gegebenen Datensatz und können zum Beispiel die Häufigkeiten, mit denen bestimmte Wortfolgen oder Muster auftreten, erkennen.\ct{baeldung_ngram}{Abs. 22}

Der einfachste Ansatz für ein statistisches Modell ist das \term{N-Gramm}: Hierbei werden die Wahrscheinlichkeiten aller -- im Datensatz enthaltenen -- Wortfolgen von $N$ Wörtern festgestellt, sodass bei einer vorangegangenen Anzahl von $N-1$ Wörtern das nächste Wort, das im Datensatz am häufigsten nach diesen Wörtern stand, mit einer bestimmten Wahrscheinlichkeit vorhergesagt werden kann.\ctm{\cite[Folie 20--25]{tuchemnitz_sprachmodellierung}\cite[Abs. 5, Punkt 1]{computerweekly_sprachmodellierung}}

Dabei kommen folgende Ausprägungen am meisten zur Anwendung:\ct{baeldung_ngram}{Abs. 19}

\begin{itemize}

    \item \term{Unigramme (1-Gramme)} -- wie häufig kommt ein einziges Wort vor\ctm{\cite[Folie 22]{tuchemnitz_sprachmodellierung}\cite[Abs. 3, Punkt 1]{medium_ngrams_in_nlp}}

    \item \term{Bigramme (2-Gramme)} -- wie häufig folgt ein Wort auf ein anderes\ctm{\cite[Folie 24--25]{tuchemnitz_sprachmodellierung}\cite[Abs. 3, Punkt 2]{medium_ngrams_in_nlp}} 

    \item \term{Trigramme (3-Gramme)} bis \term{5-Gramme} -- wie oft folgt auf eine Kette von Wörtern ein anderes\ctm{\cite[Folie 28]{tuchemnitz_sprachmodellierung}\cite[Abs. 3, Punkt 3--4]{medium_ngrams_in_nlp}}
    
\end{itemize}

Längere Ketten von Wörtern führen zu exponentiell mehr Kombinationsmöglichkeiten, erfordern ebenso viel mehr Trainingsdaten und bringen außerdem durch das N-Grammen generell fehlende Kontextverständnis keine nennenswerten Vorteile mehr.\ct{dataleap_geschichte_sprachmodellierung}{Abs. 2,8--9}

Es gibt viele Abwandlungen, die verschiedene Schwächen von N-Grammen korrigieren sollen, wie beispielsweise bidirektionale oder exponentielle N-Gramme, die \textsl{Häufigkeiten von sowohl folgenden als auch vorangehenden Wörtern zählen}\ct{computerweekly_sprachmodellierung}{Abs. 5, Punkt 2} beziehungsweise die zusätzliche Mechanismen in die Wahrscheinlichkeitsrechnung einbauen, die den Kontext mehr berücksichtigen und zu schnelle Rückschlüsse aus den Daten verhindern sollen.\ctm{\cite[Abs. 5, Punkt 2]{computerweekly_sprachmodellierung}\cite[Folie 10]{oxford_maximum_entropy_modelling}}

Ein weiterer sehr populärer, aber deutlich komplexerer Ansatz ist das \term{neuronale Netz}: Vereinfacht gesagt werden die Parameter einer Reihung von Matrixmultiplikationen -- den Neuronen und Synapsen in unserem Gehirn nachgebildet --\ctm{\cite[Abs. 2]{pangeanic_neuronale_netze}\cite[Folie 16--21]{hsmannheim_grundlagen_neuronale_netze}} mit einer idealerweise extrem großen Menge an Trainingsdaten so lange angepasst, bis aus einer kodierten Wortfolge die Wahrscheinlichkeitsverteilung des darauf folgenden Wortes \emph{errechnet} werden kann.\ctm{\cite[Abs. 2--4,Formel 3]{cornellbowers_neural_networks_matrix_multiply}\cite[Abs. 3--7]{pangeanic_neuronale_netze}}

Diesem Algorithmus sind \emph{in der Theorie} keine Grenzen gesetzt, sowohl für die Länge der Wortfolge als auch für die Anzahl der Parameter als auch für die Genauigkeit der Ausgabe.\ctm{\cite[Abs. 1]{liquidnews_ki_grenzenlos}\cite[Abs. 5]{visusadvisory_starke_schwache_ki}\cite[Abs. 1--3]{ibm_artificial_general_intelligence}} \emph{In der Praxis} erreichen Modelle allerdings nie perfekte Quoten,\ctm{\cite[Abs. 1--6]{medium_escaping_trap_local_minima}\cite[Abs. 2--6]{futurism_openai_losing_money_on_chatgpt}} sicher auch, da ein gewisses finanzielles Interesse Unternehmen davon abhält, \textsl{häufig kostenlos oder unbegrenzt Benutzern zur Verfügung gestellten}\ctm{\cite[\qt{Free}/\qt{Pro} Abonnement]{openai_chatgpt_pricing}\cite[\qt{Paraphrasierung}]{languagetool_premium}} Modellen auch nur annähernd genug Rechenkapazität zu geben.

Bevor die größten Modelle in ihrer Größe jedoch weiter wachsen, fehlt es als Erstes an einer ganz anderen Ressource: Trainingsdaten, besonders hochqualitativen.\ctm{\cite[Abs. 1--2,28--29]{medium_solution_insufficient_llm_training_data}\cite[Abs. 1--2,37,51]{linkedin_chatgpt_consumed_internet}}

Trotzdem sind neuronale Netze älteren Modellen wie N-Grammen weit überlegen, weil sie Kontext in einer Menge und Art berücksichtigen können, die mit N-Grammen realistisch nicht erreichbar ist,\ctm{\cite[Folie 46--49]{tuchemnitz_sprachmodellierung}\cite[Abs. 1,29]{towardsdatascience_neural_networks_over_ngram}} selbst wenn theoretisch ein solches System mithilfe zusätzlicher Regeln erschaffen werden könnte.\ct{geeksforgeeks_rule_based_nlp}{Abs. 1,4--5,8,10}

\subsubsection{Wissensbasierte Sprachmodelle}
\label{sec:gegenwart:algorithmen_und_modelle:wissensbasierte_sprachmodelle}

Algorithmen aus dieser Kategorie verlassen sich regelmäßig auf handgefertigte Regelsätze,\ct{tuchemnitz_sprachmodellierung}{Folie 4} können diese aber auch größtenteils selbstständig aus vorhandenen hochqualitativen Daten ableiten.\ctm{\cite[Abs. 6, Punkt 2]{geeksforgeeks_rule_based_system_in_ai}\cite[Abs. 5, Punkt 6]{onlinetutorialhub_rule_based_nlp}} Ein prominentes Beispiel ist hier LanguageTool, ein Rechtschreib- und Grammatikkorrektur-Programm, das sich neben einer neueren Künstlichen Intelligenz vor allem auf einen großen Regelkorpus stützt.\ctm{\cite[14--15]{languagetool_kuenstliche_intelligenz_bessere_korrektur}\cite{languagetool_community_error_rules}}

Regelbasierte Methoden sind heute allerdings nicht mehr wirklich relevant.\ctm{\cite[Abs. 14]{languagetool_kuenstliche_intelligenz_bessere_korrektur}\cite[Abs. 4,6,9--15]{kitrainingszentrum_regelbasierte_ki_vs_llm}} Es gibt zu viele Sprachen, zu viele Regeln und vor allem zu viele Sonderfälle. Wissensbasierte Sprachmodelle können \emph{in der Theorie} perfekte oder -- abhängig vom Anwendungsfall -- zumindest die besten Voraussagen leisten,\ctm{\cite[Abs. 6,13--14]{onlinetutorialhub_rule_based_nlp}\cite[Abs. 22]{geeksforgeeks_rule_based_system_in_ai}} sind jedoch \emph{in der Praxis} sowohl im Aufbau als auch in der Instandhaltung und Pflege schlicht zu aufwändig.\ctm{\cite[Abs. 4,17]{onlinetutorialhub_rule_based_nlp}\cite[Abs. 23]{geeksforgeeks_rule_based_system_in_ai}}

% ---------------- Herausforderungen ---------------
\subsection{Herausforderungen}
\label{sec:gegenwart:herausforderungen}

Auf jeden Fall steht fest, dass das Hauptaugenmerk bei der Verbesserung \emph{eines} der beiden Konzepte auf der Sprachkorrektur liegen sollte, weil Fehler dort eine schlimmere Auswirkung haben: Vorschläge gibt es mehrere und sie sind optional, das heißt, der Benutzer kann immer noch entscheiden, sie nicht auszuwählen.\ctm{\cite[Abs. 1]{techbone_android_vorschlaege_verwenden_einstellen}\cite[Abs. 6--7]{apple_autokorrektur_vorschlaege_verwenden}} Autokorrektur hingegen arbeitet im Hintergrund und kann entweder alles überprüfen und möglicherweise korrigieren oder gar nichts.\ctm{\cite[Abs. 2]{apple_autokorrektur_vorschlaege_verwenden}\cite[Abs. 2]{makeuseof_turn_on_off_autocorrect_android}}

Drei große Herausforderungen, vor denen die Entwickler der Sprachkorrektur stehen, sind:

\begin{description}

    \item[Kontext] Der Algorithmus muss sowohl den Satz, den Paragrafen und den gesamten Text als auch den genauen \usfw{Softwarekontext} kennen. Während Ersteres zwar grundsätzlich schon durch neuronale Netze mehr oder weniger erfolgreich angegangen wird,\ctm{\cite[Abs. 5, Punkt 5]{computerweekly_sprachmodellierung}\cite[Abs. 1,3]{geeksforgeeks_large_language_model}} braucht es für Letzteres erst einmal eine genormte, allgemein akzeptierte und implementierte technische Grundlage --- kein einziges Adjektiv trifft bis jetzt zu, selbst wenn die \textsl{Android-API eine technische Möglichkeit für Apps seit 2009 bereitstellt}\ctsr{androiddeveloper_api_inputmethod_editorinfo_fieldid, androiddeveloper_api_inputmethod_editorinfo_fieldname}. Dabei wird so gut wie jede Person zum Beispiel an verschiedene andere Personen auf verschiedene Arten Nachrichten schreiben und wieder anders digitales Tagebuch führen.

    \item[Mischsprache] Viele Nutzer berichten in den sozialen Medien, dass sie häufig mehrere Sprachen in einem Text vereinen und Korrektur dabei viele Probleme verursacht.\ctsr{reddit_autocorrect_multiple_languages_1, reddit_autocorrect_multiple_languages_2, reddit_autocorrect_multiple_languages_3, reddit_autocorrect_multiple_languages_4} Der Korrekturalgorithmus steht dort vor einer besonders komplizierten Aufgabe, denn Wörterbücher verschiedener Sprachen lassen sich sicher vereinen,\ct{microsoft_swiftkey_mit_mehreren_sprachen}{Abs. 1} Grammatik und Kontext sind allerdings deutlich schwieriger zu kombinieren, schließlich sind Vorhersagemodelle für jede Sprache einzeln trainiert -- man muss für jede neue Sprache ein Sprachpaket inklusive Wörterbuch und Sprachmodell herunterladen.\ct{microsoft_swiftkey_mit_mehreren_sprachen}{Abs. 6--12} Im Regelfall wird hier immer gerade das Modell verwendet, dessen Sprache der Algorithmus dort vermutet,\ctm{\cite[Abs. 2]{microsoft_swiftkey_mit_mehreren_sprachen}\cite[Abs. 6]{chromium_adaptive_spell_checking_multilingual}} was allerdings nur dann wirklich gut funktioniert, wenn pro Satz eine Sprache verwendet wird.\ct{chromium_adaptive_spell_checking_multilingual}{Abs. 7,15--17}

    \item[Schreibstil] Individuelle Wörter, Abkürzungen oder Wendungen, die nicht im gewöhnlichen oder durchschnittlichen Sprachgebrauch in der Form oder Häufigkeit vorkommen, sollen zwar durch lernfähige Methoden berücksichtigt werden,\ctm{\cite{google_making_touchscreen_keyboards_adaptive}\cite[Abs. 1--2]{androidpolice_gboard_personal_dictionary}\cite[Abs. 3--4,39,41,49,52]{linkedin_surveilling_users_adaptive_soft_keyboards}} das muss aber nicht immer so funktionieren; dabei kann sowohl zu wenig als auch zu viel oder nicht in den richtigen Momenten Rücksicht genommen werden, wie verschiedene Nutzer berichten.\ctsr{reddit_autokorrect_schlecht_1,reddit_autokorrect_schlecht_2}

\end{description}

Die letzte, vergleichsweise kleine Herausforderung liegt in dem Wechsel von externer Berechnung, der Abhängigkeit von der Cloud, hin zur lokalen Berechnung.\ct{androidcentral_ondevice_ai_processing}{Abs. 1--5} Das hat verschiedene Vorteile, wie beispielsweise mehr Privatsphäre, schnellere Ergebnisse und Unabhängigkeit von der Internetverbindung,\ctm{\cite[Abs. 7, Punkt 1--3]{androidcentral_ondevice_ai_processing}\cite[Abs. 8, Punkt 1--3]{microsoft_ondevice_ai_windows_ai_foundry}} gleichzeitig aber auch Nachteile: Große Sprachmodelle, wie sie heute benutzt werden, können technisch bedingt nicht auf schwachen Konsumentenmobilgeräten laufen; das Verkleinern verringert die Genauigkeit der Ergebnisse.\ct{phonearena_ultimate_guide_smartphone_ai}{Abs. 23--26}

% ===================== Zukunft ====================
\section{Zukunft und mögliche Lösungen}
\label{sec:zukunft_und_moegliche_loesungen}

\subsection{Kontext}
\label{sec:zukunft_und_moegliche_loesungen:kontext}

Damit Algorithmen sich besser an den Textkontext anpassen können, lässt sich nicht viel ändern: \textsl{Künstliche Intelligenz wird immer weiter erforscht werden, neuronale Netze werden weiter wachsen}\ctm{\cite[Abs. 11,13--15]{theneuralwire_evolution_neural_networks}\cite[Abs. 50,54--55]{codewave_history_development_neural_networks}} und die ebenfalls \textsl{noch wachsende Prozessorleistung}\ct{mit_death_of_moores_law}{Abs. 2} wird immer mehr möglich machen, sowohl auf großen Servern als auch auf Mobilgeräten.

Das alles ist allerdings nichts Weltbewegendes, die Fortschrittskurve wird -- dem jetzigen Trend folgend --, wie in sehr vielen technischen Bereichen,\ct{medium_technologys_favorite_s_curve}{Abs. 1} immer weiter abflachen. Der Benutzer wird nur graduell leicht bessere und genauere Vorschläge und Korrekturen erleben. Vielleicht nehmen wir automatische Textkorrektur auch nur -- durch unsere natürliche Neigung zu Negativem beeinflusst --\ct{toolifyai_uncovering_frustations_of_autocorrect}{Abs. 4--5} als schlechter wahr, als sie eigentlich ist, und sind unsere Ansprüche an ein Werkzeug wie Autokorrektur, das jedes einzelne Wort unserer Texte abändern kann, nicht weniger als perfekt.\ctm{\cite[Abs. 1--3,19--21]{tomsguide_autocorrect_turned_off_test}\cite[Abs. 17]{styleblueprint_when_autocorrect_goes_wrong}}

Der \usfw{Softwarekontext} ist ebenfalls schwer zu berücksichtigen: App-Entwicklern ist auf allen Betriebssystemen derartiger Freiraum gelassen, dass ihnen eine einheitliche technische Erkennungskomponente für Eingabefelder bei vielen Apps wohl gar nicht aufzuzwingen wäre. Beispiele, die viele Fragen aufwerfen, sind hier:

\begin{itemize}

    \item Textnachrichten-Apps: Schreibt man anders an denselben Benutzer in verschiedenen Apps? Wie erkennt man denselben Benutzer über Apps hinweg?
    
    \item Notizen-Apps: Um was für eine Art von Text (Tagebuch, Aufsatz, Satire) handelt es sich? Wie unterschiedlich schreibt man in unterschiedlichen Textarten?
    
    \item Spiele mit Texteingabe: Braucht man überhaupt Autokorrektur oder Vorschläge in Spielen, wie zum Beispiel Wortspielen oder Spielen mit Textnachrichten-Funktion? Wie erkennt man, wann man das tut und wann nicht?
    
    \item Social-Media-Apps: Wie differenziert man zwischen privaten und öffentlichen, adressierten und nicht adressierten Nachrichten? Wie schreibt man an verschiedene Gruppen von Personen?
    
\end{itemize}

Es wäre zwar durchaus möglich, jedem Eingabefeld von der Betriebssystemseite aus eine einzigartige Identifikation zu geben, dieses Problem scheint aber erstens gar nicht von Android in irgendeiner Form verfolgt zu werden und zweitens nicht unbedingt eine Lösung zu sein, da Tastaturen dann, ohne mehr Informationen über jedes Eingabefeld als dessen Identifikation und die zugehörige App zu kennen, eine wahrscheinlich unnötig starke Separierung verschiedener \usfw{Softwarekontexte} aufgrund mangelnder erkennbarer Zusammenhänge vornehmen müssten, die dem Nutzer letztendlich nicht zugutekäme, es sei denn, er würde in jedem dieser Kontexte sehr ausführlich und angemessen vielfältig schreiben.

\subsection{Mischsprache}
\label{sec:zukunft_und_moegliche_loesungen:mischsprache}

Diese Herausforderung hat eine deutlich einfachere -- wenn auch nicht einfache -- Lösung als das Kontextproblem: Weil dieses Feld schlicht noch zu unentwickelt ist, fehlt es an guten, \emph{wirklich} multilingualen Algorithmen, die vor allen Dingen mit realen oder zumindest sehr realistischen Daten in einer Vielzahl von Arten, in denen Nutzer Sprachen benutzen und kombinieren, trainiert werden müssen.

Der momentane Ansatz, die Sprache für jeden Satz oder Paragrafen zu bestimmen und dann auf der vermeintlich erkannten einzelnen Sprache zu beharren, ist für den geschäftlichen Kontext vielleicht in Ordnung, da hier sehr regelkonform und formal mehrere Sprachen im Sinne der Internationalität in einem Text kombiniert werden, ist jedoch nach den Nutzerberichten nicht unbedingt für den privaten Gebrauch geeignet.

Aus diesem Grund ist für die multilingualen Algorithmen, an denen zumindest Google schon arbeitet,\ct{chromium_adaptive_spell_checking_multilingual}{Abs. 4,16} wichtig, dass sie in intensiver Zusammenarbeit mit der gesamten oder zumindest einer großen und sehr repräsentativen Gruppe betroffener Nutzer, beziehungsweise -- in Google-Manier -- auf Basis ihrer gesamten Nutzerdaten entwickelt werden, um die größte Zielgruppe sicher zu befriedigen.

\subsection{Schreibstil}
\label{sec:zukunft_und_moegliche_loesungen:schreibstil}

Die Anpassungsfähigkeit der Tastaturen an persönliche Schreibstile lässt sich auf ganz ähnliche Weise wie die an Mischsprachen verbessern: Der Einbezug der betroffenen Benutzer oder ihrer Daten ist -- meiner Ansicht nach -- elementar für die Entwicklung von Algorithmen, die solch chaotisches, undefiniertes und unlogisches Schreibverhalten vorhersagen können sollen.

Unter dem Gesichtspunkt der Privatsphäre lassen sich sicherlich einige Bedenken hinsichtlich der vorgeschlagenen Vorgehensweise äußern, die bereits existierenden Konzepte für anonyme, föderierte Entwicklung und konstante Anpassung werden diese jedoch schnell beseitigen:\ct{getmaxim_privacy_preserving_autocorrect_gboard}{Abs. 25}

Google hat das Problem bereits vor langer Zeit zumindest größtenteils gelöst. GBoard, die Bildschirmtastatur von Google, nutzt \usfw{federated learning} und \usfw{differential privacy}, um gleichzeitig so privat, individuell und generell wie möglich ihren Vorhersage- und Korrekturalgorithmus zu trainieren.\ctm{\cite[Abs. 5--7,14]{getmaxim_privacy_preserving_autocorrect_gboard}\cite[Abs. 2--5]{google_federated_learning}\cite[Abs. 10,12--15]{google_private_federated_learning_gboard}} Das einzige Problem stellt die Antiproportionalität von Individualität und Privatsphäre dar: Die Glättung von zu individuellen und somit identifizierbaren Daten schützt zwar die Privatsphäre, wirkt sich aber negativ auf die Fähigkeit des Modells aus, genau diese individuellen Merkmale lernen vorherzusagen.\ctm{\cite[Abs. 15--22]{getmaxim_privacy_preserving_autocorrect_gboard}\cite[Abs. 9--11]{google_federated_learning}\cite[Abs. 4--7,12]{google_private_federated_learning_gboard}}

Hier werden dann künstlich erzeugte Trainingsdaten dazu benutzt, um auf anonymem Wege Nutzen aus den normalerweise versteckten, zu individuellen Daten zu ziehen, indem große Datensätze künstlicher falscher Texte und Korrekturen mithilfe großer Sprachmodelle generiert werden, womit wiederum neue, kleinere Sprachmodelle trainiert werden. Diese Modelle werden dann -- vereinfacht gesagt -- auf Nutzerbasis fein angepasst, jedoch so, dass sich keine individuellen Beiträge bemessen lassen.\ct{getmaxim_privacy_preserving_autocorrect_gboard}{Abs. 23--46}

\subsection{Lokalisierung}
\label{sec:zukunft_und_moegliche_loesungen:lokalisierung}

Die Lokalisierung stellt gewissermaßen gar keine Herausforderung mehr dar. Es handelt sich grundsätzlich nur noch um eine Frage der Zeit, bis viele KI-Anwendungen auf den meisten Mobilgeräten laufen können;\ct{androidcentral_ondevice_ai_processing}{Abs. 1--3,12--15} die neuesten Google-Pixel-Modelle unterstützen sogar schon Googles Gemini-Nano-Modell, eine deutlich verkleinerte Version des großen Modells,\ct{androidauthority_gemini_nano}{Abs. 3--4} darin, lokal zu laufen.\ct{google_gemini_nano_pixel_10_updates}{Abs. 2}

Eine große Rolle werden auch für andere Anwendungen kleine Sprachmodelle und ihre Gegenstücke in anderen Bereichen der Künstlichen Intelligenz spielen, die zum Beispiel mithilfe größerer Modelle für spezifische Anwendungszwecke trainiert werden können und dabei ihre \textit{Vorbilder} sogar übertreffen.\ct{datacamp_small_language_models}{Abs. 1--2,16--18}

Regelbasierte Ansätze sind zwar in der Hinsicht, dass sie meist keine so großen Mengen Arbeitsspeicher benötigen, wie Sprachmodelle es tun,\ct{uniteai_understanding_llm_parameters_memory_requirements}{Abs. 31--36,62--68} gut dafür geeignet, auf Mobilgeräten zu laufen, wirklich gute Modelle erfordern hier aber -- ähnlich wie große statistische Sprachmodelle --\ctm{\cite[Abs. 1--2]{mindverse_speicheranforderungen_grosser_sprachmodelle}\cite[Abs. 27--30,62--68]{uniteai_understanding_llm_parameters_memory_requirements}} eine sehr große Menge an festem Speicher, da die verschiedensten sprachlichen Regeln und \usfw{Hilfsmittel}, wie beispielsweise Ontologien\ct{aaron_ontologie_verbessert_antwortqualitaet_llm}{Abs. 1, 3--5}, Wissensgraphen\ct{ibm_knowledge_graph}{Abs. 1, 4--5,8--9} oder \textsl{große N-Gramm-Sammlungen}\ct{google_books_ngrams_datasetsv3}{≈ 1,29 TB}, teilweise große Datensätze der Vollständigkeit halber, also um tatsächlich richtig gut zu sein, benötigen.

% ================= Zusammenfassung ================
\section{Zusammenfassung}
\label{sec:zusammenfassung}

Oberflächlich ist an der Sprachkorrektur und -vorhersage erst einmal nicht viel auffällig. Auch im Rahmen dieser Arbeit ist es nicht möglich, über eine grobe Betrachtung hinauszukommen, denn die erläuterten Konzepte sind sehr weit gefasst und umfassen ihrerseits viele weitere Konzepte: Besonders Neuronale Netze, Künstliche Intelligenz und \usfw{machine learning} sind in einer aufwärts strebenden Entwicklungsphase und versprechen neben einem großen Einfluss auf die globale Zukunft auch der Wissenschaft und Forschung noch viele neue Ideen, die zu einigen der wichtigsten teilweise erst jüngst entdeckten dazukommen.

Regelbasierte Sprachmodelle sind nur in ganz besonderen Fällen tatsächlich noch ein entscheidender Faktor in Systemen und eher eine Sache der Vergangenheit, obwohl sie eine theoretische Perfektion versprechen, der sich zumindest keine \emph{K}I nähern kann.

Alles in allem ist \term{Kontext} das wichtigste Wort dieses ganzen Themenbereichs: Alles richtet sich nur noch danach, den Kontext so ganzheitlich, aber auch nuanciert wie möglich zu erfassen und darauf basierend zu handeln. \usfw{LLMs}, \usfw{embedding vectors} und \usfw{attention layers} spielen dabei eine sehr große Rolle.

% =========== Ausblick auf die Facharbeit ==========
\section{Ausblick auf die Facharbeit}
\label{sec:ausblick_auf_die_facharbeit}

% ---- Problemstellung und mögliche Zielsetzung ----
\subsection{Problemstellung und mögliche Zielsetzung}
\label{sec:ausblick_auf_die_facharbeit:problemstellung_und_moegliche_zielsetzungen}

Ich plane für meine Facharbeit, ein sehr persönliches Problem zu behandeln: Alle bekannten und beliebten Bildschirmtastaturen sind nicht quelloffen und ganz sicher nicht kostenlos -- man zahlt nur meistens nicht mit Geld, sondern Informationen. Die Tastaturen, die quelloffen sind, sind im Aussehen -- wenn überhaupt -- nicht weit von den anderen entfernt, welche Funktionen ich bei ihnen aber besonders vermisse, sind gute Sprachkorrektur und -vorhersage.

Grund dafür ist ziemlich sicher das ambivalente Verhältnis von vielen Nutzern, die viele Daten produzieren, und einer Tastatur, die nur durch viele Daten besser und bekannter wird. Ein weniger privatsphärefreundlicher Ansatz spielt sicher auch der großen Datenbasis der Superkonzerne Microsoft, Google und Apple zu.

Eine mögliche Zielsetzung für die folgende Facharbeit ist also die Entwicklung eines Prototyps zur Sprachkorrektur und -vorhersage am Beispiel von Bildschirmtastaturen. Dabei soll nicht unbedingt das Niveau der führenden Algorithmen erreicht werden -- das wäre unrealistisch --, aber möglicherweise das der quelloffenen.

Neben diesem ambitionierten Ziel steht allerdings auch die grundsätzliche Auseinandersetzung mit alten und neuen, aber hauptsächlich den erfolgreichsten Ansätzen der Sprachkorrektur- und -vorhersagealgorithmen und die Erforschung, ob die Theorie in der Praxis bestätigt oder entkräftet wird.

% ------------ Evaluation und Bewertung ------------
\subsection{Evaluation und Bewertung}
\label{sec:ausblick_auf_die_facharbeit:evaluation_und_bewertung}

Evaluiert werden könnte ein solcher Algorithmus beispielsweise anhand von:

\begin{itemize}

    \item Test-Datensätzen, in die zufällige Fehler eingearbeitet worden sind,

    \item Vergleichstests mit anderen Algorithmen von zum Beispiel den quelloffenen Bildschirmtastaturen oder aber den großen, wobei komplexer, oder

    \item benutzergesteuerter Testung und Bewertung, indem Testpersonen den Algorithmus nutzen sollen, um einen Test damit zu absolvieren.
    
\end{itemize}

% ---------------- Problemabgrenzung ---------------
\subsection{Problemabgrenzung}
\label{sec:ausblick_auf_die_facharbeit:problemabgrenzung}

Der zu entwickelnde Algorithmus soll keine großen oder \textit{kleinen} Sprachmodelle, die trotzdem ein paar Milliarden Parameter haben, einsetzen, insbesondere da deren Entwicklung sehr zeit- und arbeitsaufwändig ist. Neuronale Netze generell kann ich noch nicht ausschließen, da sie möglicherweise noch wichtig werden, wenn es darum geht, riesige N-Gramm-Datensätze für die Vorhersage auf Mobiltelefonen zu verwenden und \textit{Platz gespart} werden muss.

Wie bereits erwähnt, soll der Algorithmus auch nicht schon seit langer Zeit entwickelte und verbesserte Algorithmen schlagen, sondern vielleicht eher neue, möglicherweise unkonventionelle Ansätze erproben und alten gegenüberstellen.

Sprachverarbeitung ist schließlich ein sich rasant weiterentwickelndes Feld, in dem mit großen Ambitionen geforscht und in das vor allen Dingen viel Geld investiert wird. Neu entwickelte Technologien können ältere schnell übertreffen und für den Normalgebrauch irrelevant machen.

Meine Facharbeit wird also eine Art Balance aus neueren und älteren Methoden anstreben und untersuchen, inwiefern die älteren immer noch anwendbar und mit neueren Methoden kombinierbar sind.

\pagebreak
% ############## Literaturverzeichnis ##############
\phantomsection
\addcontentsline{toc}{section}{Literatur}
\bibliographystyle{alphadin}
\bibliography{bibliografie}

\pagebreak
% ############## Literaturverzeichnis ##############
\section*{Erklärungen}
\addcontentsline{toc}{section}{Erklärungen}

\subsection*{1. Versicherung der selbstständigen Anfertigung der Hausarbeit}

Hiermit erkläre ich, dass ich die vorliegende Hausarbeit selbstständig angefertigt, keine anderen als die angegebenen Hilfsmittel benutzt und die Stellen der Hausarbeit, die im Wortlaut oder im wesentlichen Inhalt aus anderen Werken entnommen wurden, mit genauer Quellenangabe kenntlich gemacht habe. Verwendete Informationen aus dem Internet sind dem Fachlehrer vollständig als Ausdruck beziehungsweise digital zur Verfügung gestellt worden.

\begin{center}

    \includegraphics[height=1cm]{Hausarbeit Unterschrift 1.png}
  
    \vspace{-0.6cm}
  
    \rule{0.6\linewidth}{0.4pt}
    
\end{center}


\subsection*{2. Veröffentlichungseinverständnis}

Hiermit erkläre ich, dass ich damit einverstanden bin, wenn die von mir verfasste Hausarbeit der schulinternen Öffentlichkeit zugänglich gemacht wird.

\begin{center}

    \includegraphics[height=1cm]{Hausarbeit Unterschrift 2.png}
  
    \vspace{-0.6cm}
  
    \rule{0.6\linewidth}{0.4pt}
    
\end{center}

\end{document}
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%